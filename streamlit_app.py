import os
import requests
import streamlit as st
import pandas as pd
from bs4 import BeautifulSoup
from collections import Counter
from nltk import ngrams
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from apify_client import ApifyClient

# === API keys ===
def get_google_results(query):
    url = "https://serpapi.com/search"
    params = {
        "engine": "google",
        "q": query,
        "gl": "pl",
        "hl": "pl",
        "api_key": SERP_API_KEY,
        "num": 5
    }

    # ğŸ” DEBUG â€“ sprawdÅº, czy klucz API siÄ™ odczytaÅ‚
    st.write("ğŸ› ï¸ API Key:", SERP_API_KEY)

    res = requests.get(url, params=params)
    results = res.json()
    st.write("ğŸ“¦ OdpowiedÅº SERP API:", results)
    return [r["link"] for r in results.get("organic_results", [])][:5]
# === Pobieranie treÅ›ci przez Apify Web Scraper ===
def extract_text(url):
    try:
        run = apify_client.actor("apify/web-scraper").call(run_input={
            "startUrls": [{"url": url}],
            "pageFunction": """async function pageFunction(context) {
                return {
                    url: context.request.url,
                    text: document.body.innerText
                };
            }""",
            "proxyConfiguration": { "useApifyProxy": True }
        })
        items = apify_client.dataset(run["defaultDatasetId"]).list_items().get("items", [])
        return items[0]["text"] if items else ""
    except Exception as e:
        st.warning(f"BÅ‚Ä…d pobierania z Apify dla {url}: {e}")
        return ""

# === Generowanie n-gramÃ³w ===
def generate_ngrams(text, n):
    tokens = [t.lower() for t in text.split() if t.isalpha()]
    return [" ".join(gram) for gram in ngrams(tokens, n)]

# === STREAMLIT INTERFEJS ===
st.title("ğŸ” Top5Grams â€“ analiza n-gramÃ³w z top 5 wynikÃ³w Google")

query = st.text_input("Wpisz sÅ‚owo kluczowe", placeholder="np. audyt SEO")
if st.button("Analizuj") and query:
    with st.spinner("Pobieram dane..."):
        urls = get_google_results(query)
        st.write("ğŸ”— Wyniki Google:", urls)

        all_text = ""
        for url in urls:
            st.write(f"ğŸ“„ Pobieram z: {url}")
            tekst = extract_text(url)
            st.write(f"âœ… DÅ‚ugoÅ›Ä‡ treÅ›ci: {len(tekst)} znakÃ³w")
            all_text += tekst + " "

        ngram_data = []
        for n in [1, 2, 3]:
            ngram_list = generate_ngrams(all_text, n)
            freq = Counter(ngram_list)
            for k, v in freq.items():
                ngram_data.append({"n-gram": k, "typ": f"{n}-gram", "liczba wystÄ…pieÅ„": v})

    if ngram_data:
        df = pd.DataFrame(ngram_data).sort_values("liczba wystÄ…pieÅ„", ascending=False)
        st.success("Gotowe! âœ…")
        st.subheader("ğŸ“Š NajczÄ™stsze n-gramy")
        st.dataframe(df.head(50), use_container_width=True)
        st.download_button("ğŸ“¥ Pobierz CSV", df.to_csv(index=False), "ngrams.csv", "text/csv")

        st.subheader("â˜ï¸ Word Cloud")
        wc = WordCloud(width=800, height=400, background_color="white").generate(" ".join(df["n-gram"]))
        fig, ax = plt.subplots()
        ax.imshow(wc, interpolation="bilinear")
        ax.axis("off")
        st.pyplot(fig)
    else:
        st.error("âŒ Nie udaÅ‚o siÄ™ pobraÄ‡ treÅ›ci z Å¼adnej strony. SprÃ³buj inne sÅ‚owo kluczowe.")
