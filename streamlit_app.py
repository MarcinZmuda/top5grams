import os
import requests
import streamlit as st
import pandas as pd
from collections import Counter
from nltk import ngrams
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from apify_client import ApifyClient

# === API Keys ===
SERP_API_KEY = os.getenv("SERPAPI_API_KEY")
APIFY_API_TOKEN = os.getenv("APIFY_API_TOKEN")
apify_client = ApifyClient(APIFY_API_TOKEN)

# === Pobierz top 5 wynik√≥w z Google ===
def get_google_results(query):
    url = "https://serpapi.com/search"
    params = {
        "engine": "google",
        "q": query,
        "gl": "pl",
        "hl": "pl",
        "api_key": SERP_API_KEY,
        "num": 5
    }
    res = requests.get(url, params=params)
    results = res.json()
    return [r["link"] for r in results.get("organic_results", [])][:5]

# === Pobierz tekst z ka≈ºdej strony przez Apify Web Scraper ===
def extract_text(url):
    try:
        run = apify_client.actor("apify/web-scraper").call(run_input={
            "startUrls": [{"url": url}],
            "pageFunction": """async function pageFunction(context) {
                return {
                    url: context.request.url,
                    text: document.body.innerText
                };
            }""",
            "proxyConfiguration": { "useApifyProxy": True }
        })

        list_page = apify_client.dataset(run["defaultDatasetId"]).list_items()
        items = list_page.items()

        for item in items:
            if "text" in item and item["text"].strip():
                return item["text"]

        return ""
    except Exception as e:
        st.warning(f"B≈ÇƒÖd pobierania z Apify dla {url}: {e}")
        return ""

# === Generowanie n-gram√≥w ===
def generate_ngrams(text, n):
    tokens = [t.lower() for t in text.split() if t.isalpha()]
    return [" ".join(gram) for gram in ngrams(tokens, n)]

# === STREAMLIT INTERFEJS ===
st.title("üîé Top5Grams ‚Äì analiza n-gram√≥w z top 5 wynik√≥w Google")

query = st.text_input("Wpisz s≈Çowo kluczowe", placeholder="np. audyt SEO")
if st.button("Analizuj") and query:
    with st.spinner("Pobieram dane..."):
        urls = get_google_results(query)

        all_text = ""
        for url in urls:
            st.write(f"üìÑ Pobieram z: {url}")
            tekst = extract_text(url)
            st.write(f"‚úÖ D≈Çugo≈õƒá tre≈õci: {len(tekst)} znak√≥w")
            all_text += tekst + " "

        ngram_data = []
        for n in [1, 2, 3]:
            ngram_list = generate_ngrams(all_text, n)
            freq = Counter(ngram_list)
            for k, v in freq.items():
                ngram_data.append({
                    "n-gram": k,
                    "typ": f"{n}-gram",
                    "liczba wystƒÖpie≈Ñ": v
                })

    if ngram_data:
        df = pd.DataFrame(ngram_data).sort_values("liczba wystƒÖpie≈Ñ", ascending=False)
        st.success("Gotowe! ‚úÖ")
        st.subheader("üìä Najczƒôstsze n-gramy")
        st.dataframe(df.head(50), use_container_width=True)

        st.download_button("üì• Pobierz CSV", df.to_csv(index=False), "ngrams.csv", "text/csv")

        st.subheader("‚òÅÔ∏è Word Cloud")
        wc = WordCloud(width=800, height=400, background_color="white").generate(" ".join(df["n-gram"]))
        fig, ax = plt.subplots()
        ax.imshow(wc, interpolation="bilinear")
        ax.axis("off")
        st.pyplot(fig)
    else:
        st.error("‚ùå Nie uda≈Ço siƒô pobraƒá tre≈õci z ≈ºadnej strony. Spr√≥buj inne s≈Çowo kluczowe.")
